{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4766ebb-432c-4bdb-8050-23df797098be",
   "metadata": {},
   "source": [
    "<!-- # Langchain은 다양한 LLM(대규모 언어 모델)을 지원한다\n",
    "-\t대규모 언어 모델(LLM, Large Language Model)을 개발하는 회사들은 사용자가 자신의 애플리케이션에서 LLM을 손쉽게 활용할 수 있도록 API(Application Programming Interface) 서비스를 제공하고 있다.\n",
    "-\t하지만 각 LLM은 고유한 API 호출 라이브러리(Library)를 제공하기 때문에, 개발자는 동일한 작업을 수행하더라도 LLM에 따라 다른 코드를 작성해야 하는 번거로움이 있다.\n",
    "-\tLangchain은 이러한 문제를 해결하기 위해 다양한 LLM의 API를 통합적으로 지원한다.\n",
    "-\t여러 LLM을 동일한 인터페이스(interface)로 호출할 수 있게 하여 특정 모델에 종속되지 않도록 하고, 필요에 따라 쉽게 다른 모델로 전환할 수 있다.\n",
    "-\tLangchain이 지원하는 주요 LLM 목록\n",
    "    - https://python.langchain.com/docs/integrations/chat/#featured-providers -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e3b65a-1b4e-4fcc-99ac-ec7624dac9ad",
   "metadata": {},
   "source": [
    "## 설치\n",
    "```bash\n",
    "pip install langchain langchain-community  -qU\n",
    "pip install python-dotenv -qU \n",
    "pip install ipywidgets -qU\n",
    "pip install ipykernel ipywidgets\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9e5829-972a-4345-858d-e479dab7c320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.25'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9025ef1-ede0-4f3b-b8fe-1b9e348391ba",
   "metadata": {},
   "source": [
    "# OpenAI 모델 사용\n",
    "- https://platform.openai.com\n",
    "  \n",
    "## 결제\n",
    "1. 로그인 후 Billing 페이지로 이동.\n",
    "   - setting -> Billing\n",
    "  \n",
    "   ![openai_payment.png](figures/openai_payment.png)\n",
    "\n",
    "2. Payment methods 탭을 선택하고 카드를 등록한다. \n",
    "   \n",
    "   ![openai_payment2.png](figures/openai_payment2.png)\n",
    "\n",
    "   - 등록이 끝나면 최초 구매를 진행한다. $5 ~ $100 사이의 금액을 선택할 수 있다.\n",
    "   - 자동 충전을 설정하고 싶다면 automatic recharge 를 활성화 하고 아래 추가 설정에 입력한다. \n",
    "     - 자동 충전은 특정 금액 이하로 떨어지면 자동으로 충전한다. (**비활성화**) \n",
    "  \n",
    "   ![openai_payment3.png](figures/openai_payment3.png)\n",
    "   \n",
    "3. 수동으로 **추가 결제하기**\n",
    "   - Billing 페이지의 Overview에서 `Add to credit balance` 를 클릭한 뒤 금액을 입력하고 결제한다.\n",
    "\n",
    "## 사용량 확인\n",
    "- profile/설정 -> Usage 에서 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f99e0-d15b-4769-8d2c-b6e0a9a26237",
   "metadata": {},
   "source": [
    "## API Key 생성\n",
    "  \n",
    "![openai_create_apikey.png](figures/openai_create_apikey.png)\n",
    "\n",
    "- 로그인 -> Dashboard -> API Keys -> Create New Secreat Key\n",
    "> Settings -> API Keys\n",
    "\n",
    "## API Key 등록\n",
    "- 환경변수에 등록\n",
    "  - 변수이름: OPENAI_API_KEY\n",
    "  - 값: 생성된 키\n",
    "- dotenv를 이용해서 load\n",
    "  - Working directory에  `.env` 파일 생성하고 `OPENAI_API_KEY=생성된키` 추가한다.\n",
    "  - load_dotenv() 호출 하면 .env 파일에 있는 값을 읽은 뒤 환경변수로 등록한다.\n",
    "- **주의**\n",
    "  - 생성된 API Key는 노출되면 안된다.\n",
    "  - API Key가 저장된 파일(코드나 설정파일)이 github에 올라가 공개되서는 안된다.\n",
    "\n",
    "## 사용 비용 확인\n",
    "- settings -> Usage 에서 확인\n",
    "\n",
    "## OpenAI LLM 모델들\n",
    "-  OpenAI LLM 모델: https://platform.openai.com/docs/models\n",
    "-  모델별 가격: https://platform.openai.com/docs/pricing\n",
    "-  토큰사이즈 확인: https://platform.openai.com/tokenizer\n",
    "   -  1토큰: 영어 3\\~4글자 정도, 한글: 대략 1\\~2글자 정도\n",
    "   -  모델이 업데이트 되면서 토큰 사이즈도 조금씩 커지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5a12d-8e7d-4a29-ac91-edb62c56bfe7",
   "metadata": {},
   "source": [
    "## OpenAI 를 연동하기 위한 package 설치\n",
    "```bash\n",
    "pip install langchain-openai -qU\n",
    "```\n",
    "\n",
    "- OpenAI 자체 라이브러리 설치\n",
    "    - `pip install openai -qU`\n",
    "    - langchain-openai를 설치하면 같이 설치 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29772e79-1e31-421d-b5c1-625dc029c395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.19-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.63 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from langchain-openai) (0.3.63)\n",
      "Collecting openai<2.0.0,>=1.68.2 (from langchain-openai)\n",
      "  Downloading openai-1.84.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.63->langchain-openai) (0.3.44)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.63->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.63->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.63->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.63->langchain-openai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.63->langchain-openai) (4.14.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.63->langchain-openai) (2.11.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.63->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain-openai) (3.10.18)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: anyio in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain-openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain-openai) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain-openai) (0.16.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Downloading jiter-0.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
      "Collecting tqdm>4 (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.63->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.63->langchain-openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.63->langchain-openai) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain-openai) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/giwonjun/anaconda3/envs/lang_env/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain-openai) (2.4.0)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain-openai)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Downloading langchain_openai-0.3.19-py3-none-any.whl (64 kB)\n",
      "Downloading openai-1.84.0-py3-none-any.whl (725 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.5/725.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.10.0-cp312-cp312-macosx_11_0_arm64.whl (320 kB)\n",
      "Downloading tiktoken-0.9.0-cp312-cp312-macosx_11_0_arm64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, jiter, distro, tiktoken, openai, langchain-openai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [langchain-openai][openai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed distro-1.9.0 jiter-0.10.0 langchain-openai-0.3.19 openai-1.84.0 regex-2024.11.6 tiktoken-0.9.0 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a486e5a1-6c59-499d-a275-41060c8b0a8a",
   "metadata": {},
   "source": [
    "## OpenAI Library 를 이용한 API 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9bb769-eb14-43d5-9c26-98bb77153b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "flag=load_dotenv()\n",
    "print(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851be9d2-58e5-464b-87cb-52f09a9a146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()                                   # 환경변수에 OPENAI_API_KEY가 등록되어 있으므로 생략 가능\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",     # 발화자 - user, assistant, system\n",
    "        \"content\": \"OpenAI의 LLM 모델은 무엇이 있나요?\"\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",     # 요청할 모델\n",
    "    messages=message         # 메시지\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c7056dd-ddc5-4c0f-8653-47be64287d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI의 LLM(대형 언어 모델)에는 여러 가지 버전이 있습니다. 주요 모델은 다음과 같습니다:\n",
      "\n",
      "1. **GPT-3**: OpenAI가 2020년에 발표한 모델로, 1750억 개의 파라미터를 가지고 있습니다. 자연어 처리(NLP) 작업에서 매우 강력한 성능을 발휘합니다.\n",
      "\n",
      "2. **GPT-3.5**: GPT-3의 개선된 버전으로, 몇 가지 업데이트와 수정이 이루어진 모델입니다. 더 나은 이해력과 응답 품질을 제공합니다.\n",
      "\n",
      "3. **GPT-4**: 2023년에 발표된 최신 모델로, 다양한 작업에서 더 높은 성능을 보여줍니다. GPT-4는 멀티모달 기능을 지원하여 텍스트뿐만 아니라 이미지 입력도 처리할 수 있습니다.\n",
      "\n",
      "이 외에도 OpenAI는 API와 함께 다양한 언어 모델을 제공하여 개발자들이 다양한 애플리케이션에 활용할 수 있도록 지원합니다. 또한, ChatGPT라는 인터페이스를 통해 사용자와 상호작용하는 형태로 활용되고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)  # 응답 메시지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9ab6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "OpenAI의 LLM(대형 언어 모델)에는 여러 가지 버전이 있습니다. 주요 모델은 다음과 같습니다:\n",
       "\n",
       "1. **GPT-3**: OpenAI가 2020년에 발표한 모델로, 1750억 개의 파라미터를 가지고 있습니다. 자연어 처리(NLP) 작업에서 매우 강력한 성능을 발휘합니다.\n",
       "\n",
       "2. **GPT-3.5**: GPT-3의 개선된 버전으로, 몇 가지 업데이트와 수정이 이루어진 모델입니다. 더 나은 이해력과 응답 품질을 제공합니다.\n",
       "\n",
       "3. **GPT-4**: 2023년에 발표된 최신 모델로, 다양한 작업에서 더 높은 성능을 보여줍니다. GPT-4는 멀티모달 기능을 지원하여 텍스트뿐만 아니라 이미지 입력도 처리할 수 있습니다.\n",
       "\n",
       "이 외에도 OpenAI는 API와 함께 다양한 언어 모델을 제공하여 개발자들이 다양한 애플리케이션에 활용할 수 있도록 지원합니다. 또한, ChatGPT라는 인터페이스를 통해 사용자와 상호작용하는 형태로 활용되고 있습니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(response.choices[0].message.content)  # 응답 메시지를 Markdown 형식으로 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77416bf-be44-421b-bed1-c1da8512e983",
   "metadata": {},
   "source": [
    "## Langchain을 이용한 OpenAI API 호출\n",
    "\n",
    "- **ChatOpenAI**\n",
    "    - chat (대화-채팅) 기반 모델 model.\n",
    "    - Default 로 gpt-3.5-turbo 사용\n",
    "    - llm 전달 입력과 llm 응답 출력 타입:  Message\n",
    "> - **OpenAI**\n",
    ">     - 문장 완성 모델. (text completion) model\n",
    ">     - Default로 gpt-3.5-turbo-instruct 사용\n",
    ">       - instruct 모델만 사용가능\n",
    ">     - llm전달 입력과 llm 응답 출력 타입: str\n",
    "- Initializer 주요 파라미터\n",
    "    -  **temperature**\n",
    "        -  llm 모델의 출력 무작위성을 지정한다. \n",
    "        -  0 ~ 2 사이 실수를 설정하며 클 수록 무작위성이 커진다. 기본값: 0.7\n",
    "        -  정확한 답변을 얻어야 하는 경우 작은 값을 창작을 해야 하는 경우 큰 값을 지정한다.\n",
    "    -  **model_name**\n",
    "        -  사용할 openai 모델 지정\n",
    "    - **max_tokens**:\n",
    "        - llm 모델이 응답할 최대 token 수.\n",
    "    - **api_key**\n",
    "        - OpenAI API key를 직접 입력해 생성시 사용.\n",
    "        - API key가 환경변수에 설정 되있으면 생략한다. \n",
    "-  메소드\n",
    "    - **`invoke(message)`** : LLM에 질의 메세지를 전달하며 LLM의 응답을 반환한다.\n",
    "> - **Message**\n",
    ">     - Langchain 다양한 상황과 작업 마다 다양한 값들로 구성된 입출력 데이터를 만든다. \n",
    ">     - Langchain은 그 상황들에 맞는 다양한 Message 클래스를 제공한다. 이것을 이용하면 특정 작업에 적합한 입력값을 설정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "124af8a3-d624-4399-8062-35a6d60c63b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # .env 파일에서 환경변수 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1948856-b1e4-4da0-9311-11f798b66a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "response = model.invoke(\"OpenAI의 LLM 모델은 무엇이 있나요?\")  # 모델에 질문하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afed81fb-5249-4bd3-91b0-4f86a0a75f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='OpenAI의 LLM(대형 언어 모델)에는 여러 가지가 있습니다. 주요 모델로는 다음과 같은 것들이 있습니다:\\n\\n1. **GPT-2**: 초기의 대형 언어 모델로, 다양한 텍스트 생성 작업에 사용할 수 있습니다. 텍스트 생성, 번역, 요약 등 여러 가지 용도로 활용되었습니다.\\n\\n2. **GPT-3**: GPT-2의 후속 모델로, 인공지능 텍스트 생성 분야에서 큰 주목을 받았습니다. 1750억 개의 매개변수를 가진 이 모델은 더욱 향상된 텍스트 이해와 생성 능력을 가지고 있습니다.\\n\\n3. **GPT-3.5**: GPT-3의 개선된 버전으로, 더 나은 성능과 높은 정확성을 제공합니다. 다양한 응용 프로그램에서 활용되고 있습니다.\\n\\n4. **GPT-4**: 최신 버전으로, 이전 모델들보다 더욱 향상된 처리 능력을 가지고 있습니다. 다양한 언어 작업을 수행할 수 있도록 설계되었습니다.\\n\\n이 외에도 OpenAI는 다양한 용도로 특화된 모델과 API를 개발하고 있으며, 상황에 맞는 다양한 솔루션을 제공합니다. 각 모델은 특정한 용도와 요구에 따라 선택하여 사용할 수 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 265, 'prompt_tokens': 19, 'total_tokens': 284, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'id': 'chatcmpl-BetuFIy2VUqo9wIQOOQyykyjwXVzX', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--245dadae-ecd8-4a89-a4d7-8cffcc84984a-0' usage_metadata={'input_tokens': 19, 'output_tokens': 265, 'total_tokens': 284, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(response)  # 응답 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc22c03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI의 LLM(대형 언어 모델)에는 여러 가지가 있습니다. 주요 모델로는 다음과 같은 것들이 있습니다:\n",
      "\n",
      "1. **GPT-2**: 초기의 대형 언어 모델로, 다양한 텍스트 생성 작업에 사용할 수 있습니다. 텍스트 생성, 번역, 요약 등 여러 가지 용도로 활용되었습니다.\n",
      "\n",
      "2. **GPT-3**: GPT-2의 후속 모델로, 인공지능 텍스트 생성 분야에서 큰 주목을 받았습니다. 1750억 개의 매개변수를 가진 이 모델은 더욱 향상된 텍스트 이해와 생성 능력을 가지고 있습니다.\n",
      "\n",
      "3. **GPT-3.5**: GPT-3의 개선된 버전으로, 더 나은 성능과 높은 정확성을 제공합니다. 다양한 응용 프로그램에서 활용되고 있습니다.\n",
      "\n",
      "4. **GPT-4**: 최신 버전으로, 이전 모델들보다 더욱 향상된 처리 능력을 가지고 있습니다. 다양한 언어 작업을 수행할 수 있도록 설계되었습니다.\n",
      "\n",
      "이 외에도 OpenAI는 다양한 용도로 특화된 모델과 API를 개발하고 있으며, 상황에 맞는 다양한 솔루션을 제공합니다. 각 모델은 특정한 용도와 요구에 따라 선택하여 사용할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "843640e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,    # 생성의 다양성 조절(0~1) / gpt는 0.7 / 0은 가장 높은 답변\n",
    "    max_tokens=1000    # 최대 토큰 수\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d97fa87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "태양계를 구성하는 행성들의 이름을 태양에서 가까운 순서대로 알려줘.\n",
    "<답변형식>\n",
    "목록형식으로 답변해줘.\n",
    "- 한글이름(영어이름): 행성에 대한 간단한 설명\n",
    "\"\"\"\n",
    "res = model.invoke(prompt)  # 모델에 프롬프트 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "165ab230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 수성(Mercury): 태양계에서 가장 가까운 행성으로, 대기가 거의 없어 낮과 밤의 온도 차이가 매우 큽니다.\n",
      "- 금성(Venus): 지구와 비슷한 크기를 가진 행성이지만, 두꺼운 이산화탄소 대기로 인해 극심한 온실 효과가 발생합니다.\n",
      "- 지구(Earth): 생명체가 존재하는 유일한 행성으로, 물이 액체 상태로 존재할 수 있는 조건을 갖추고 있습니다.\n",
      "- 화성(Mars): 붉은 색을 띠는 행성으로, 과거에 물이 존재했을 가능성이 있으며, 현재 탐사 missions이 활발히 진행되고 있습니다.\n",
      "- 목성(Jupiter): 태양계에서 가장 큰 행성으로, 거대한 가스 행성이며, 강력한 자기장을 가지고 있습니다.\n",
      "- 토성(Saturn): 아름다운 고리로 유명한 행성으로, 주로 가스로 이루어져 있으며, 많은 위성을 가지고 있습니다.\n",
      "- 천왕성(Uranus): 독특하게 옆으로 기울어진 자전축을 가진 행성으로, 푸른색의 대기를 가지고 있습니다.\n",
      "- 해왕성(Neptune): 태양계에서 가장 먼 행성으로, 강한 바람과 대기에서의 폭풍이 특징입니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)  # 응답 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47bd4e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅형태 프롬프트 : role과 내용을 묶어서 전달\n",
    "prompt = [\n",
    "    (\"user\", \"클라이밍에 미친사람이 그만두게 하는 방법 알려줘.\"),             # role, 대화내용\n",
    "]\n",
    "res = model.invoke(prompt)  # 모델에 프롬프트 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3899b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클라이밍에 미친 사람을 그만두게 하는 것은 쉽지 않은 일입니다. 하지만 그들의 안전과 건강을 고려하여 접근할 수 있는 몇 가지 방법이 있습니다.\n",
      "\n",
      "1. **대화하기**: 그들의 열정에 대해 이야기하고, 클라이밍이 그들의 삶에 미치는 영향을 물어보세요. 그들이 클라이밍 외에 다른 관심사나 활동을 가질 수 있도록 유도해보세요.\n",
      "\n",
      "2. **안전 문제 강조하기**: 클라이밍의 위험성과 부상 가능성에 대해 이야기해보세요. 그들이 안전 장비를 제대로 사용하지 않거나 과도한 도전을 하는 경우, 그에 대한 우려를 표현하세요.\n",
      "\n",
      "3. **대안 제시하기**: 클라이밍 대신 할 수 있는 다른 스포츠나 취미를 제안해보세요. 예를 들어, 요가, 헬스, 자전거 타기 등 다양한 활동을 함께 해보는 것도 좋습니다.\n",
      "\n",
      "4. **사회적 활동 유도하기**: 클라이밍 외에도 친구들과 함께할 수 있는 다른 사회적 활동을 제안해보세요. 이를 통해 그들이 클라이밍 외의 즐거움을 느낄 수 있도록 도와줄 수 있습니다.\n",
      "\n",
      "5. **전문가 상담 권유하기**: 만약 그들이 클라이밍에 지나치게 집착하고 있다면, 전문가의 도움을 받는 것도 좋은 방법입니다. 상담사나 심리 전문가와의 대화를 통해 그들의 감정을 이해하고 조절할 수 있도록 도와줄 수 있습니다.\n",
      "\n",
      "이 모든 방법은 그들의 감정을 존중하며 접근해야 합니다. 강압적인 태도보다는 이해와 지지를 바탕으로 대화하는 것이 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)  # 응답 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ac3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ea95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71861354-39b6-4a7d-842d-8939d3a3e5bc",
   "metadata": {},
   "source": [
    "# Hugging Face 모델 사용\n",
    "\n",
    "## Local 에 설치된 모델 사용\n",
    "- HuggingFacePipeline 에 Model id를 전달해 Model객체를 생성한다.\n",
    "- huggingface transformers 라이브러리를 이용해 model을 생성 한 뒤 HuggingFacePipeline 에 넣어 생성한다.\n",
    "- 모델이 local에 없는 경우 다운로드 받는다.\n",
    "\n",
    "### HuggingFace 모델을 사용하기 위한 package 설치\n",
    "```bash\n",
    "pip install langchain-huggingface -qU\n",
    "```\n",
    "- nvidia GPU가 있는 경우 `torch cuda`  버전을 먼저 설치하고 `langchain-huggingface`를 설치 해야 한다. 아니면 `torch cpu` 버전이 설치된다.\n",
    "- `transformers`, `tokenizers`, `huggingface-hub` 등 huggingface 관련 package들이 같이 설치된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a459af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a6c8929-6c61-4e85-95b9-40857fd6bff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 9.05 GB, other allocations: 784.00 KB, max allowed: 9.07 GB). Tried to allocate 1.12 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFacePipeline\n\u001b[32m      3\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33mgoogle/gemma-3-1b-it\u001b[39m\u001b[33m\"\u001b[39m                               \u001b[38;5;66;03m# 1b: 파라미터수(10억), it: instruction-training => 질문-응답 형식으로 fine-tuning된 모델\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model_hf = \u001b[43mHuggingFacePipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_model_id\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# text-generation: 텍스트 생성\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_new_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# transformers.pipeline()의 설정을 하는 파라미터\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/langchain_huggingface/llms/huggingface_pipeline.py:263\u001b[39m, in \u001b[36mHuggingFacePipeline.from_model_id\u001b[39m\u001b[34m(cls, model_id, task, backend, device, device_map, model_kwargs, pipeline_kwargs, batch_size, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m     _model_kwargs = {\n\u001b[32m    260\u001b[39m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _model_kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k != \u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    261\u001b[39m     }\n\u001b[32m    262\u001b[39m _pipeline_kwargs = pipeline_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m pipeline = \u001b[43mhf_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_pipeline_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pipeline.task \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m VALID_TASKS:\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    274\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot invalid task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline.task\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    275\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcurrently only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVALID_TASKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m are supported\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    276\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/transformers/pipelines/__init__.py:1180\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1178\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mprocessor\u001b[39m\u001b[33m\"\u001b[39m] = processor\n\u001b[32m-> \u001b[39m\u001b[32m1180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:114\u001b[39m, in \u001b[36mTextGenerationPipeline.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_model_type(\n\u001b[32m    116\u001b[39m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[32m    117\u001b[39m     )\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprefix\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._preprocess_params:\n\u001b[32m    119\u001b[39m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[32m    120\u001b[39m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[32m    121\u001b[39m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[32m    122\u001b[39m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/transformers/pipelines/base.py:1016\u001b[39m, in \u001b[36mPipeline.__init__\u001b[39m\u001b[34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1011\u001b[39m     \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1012\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.device != \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m   1013\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.device, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device < \u001b[32m0\u001b[39m)\n\u001b[32m   1014\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1015\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1016\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[38;5;66;03m# If it's a generation pipeline and the model can generate:\u001b[39;00m\n\u001b[32m   1019\u001b[39m \u001b[38;5;66;03m# 1 - create a local generation config. This is done to avoid side-effects on the model as we apply local\u001b[39;00m\n\u001b[32m   1020\u001b[39m \u001b[38;5;66;03m# tweaks to the generation config.\u001b[39;00m\n\u001b[32m   1021\u001b[39m \u001b[38;5;66;03m# 2 - load the assistant model if it is passed.\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pipeline_calls_generate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.can_generate():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/transformers/modeling_utils.py:3851\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3846\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   3847\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3848\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3849\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3850\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m3851\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/torch/nn/modules/module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 9.05 GB, other allocations: 784.00 KB, max allowed: 9.07 GB). Tried to allocate 1.12 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"                               # 1b: 파라미터수(10억), it: instruction-training => 질문-응답 형식으로 fine-tuning된 모델\n",
    "model_hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task = \"text-generation\",                     # text-generation: 텍스트 생성\n",
    "    pipeline_kwargs={\"max_new_tokens\":50}         # transformers.pipeline()의 설정을 하는 파라미터\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7562fc5a-6c5f-456b-aafd-ec8dbf9ef54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model_hf.invoke(\"한국의 수도는 어디인가요?\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653f31b-82aa-462f-a140-545b5c57d485",
   "metadata": {},
   "source": [
    "# Anthropic의 Claude 모델 사용\n",
    "\n",
    "- Anthropic사의 Claude 모델은 (성능 순으로) **Haiku, Sonnet, Opus** 세가지 모델이 있다.  \n",
    "- [Anthropic사 사이트](https://www.anthropic.com/)\n",
    "- [Claude 서비스 사이트](https://claude.ai)\n",
    "- API 가격: https://docs.anthropic.com/en/docs/about-claude/pricing\n",
    "- Langchain으로 Anthropic claude 모델 사용: https://python.langchain.com/docs/integrations/chat/anthropic/\n",
    "\n",
    "## API Key 발급받기\n",
    "1. https://console.anthropic.com/ 이동 후 가입한다.\n",
    "2. 로그인 하면 Dashboard로 이동한다. Dashbord에서 `Get API Keys`를 클릭해 이동한다.\n",
    "\n",
    "![anthropic_apikey1.png](figures/anthropic_apikey1.png)\n",
    "\n",
    "1. Create key 클릭해서 API Key를 생성한다.\n",
    "\n",
    "2. 생성된 API Key를 복사한 뒤 저장. (다시 볼 수 없다.)\n",
    "   - 환경변수에 등록\n",
    "      - 변수이름: ANTHROPIC_API_KEY\n",
    "      - 값: 생성된 키\n",
    "\n",
    "## 결제 정보 등록 및 결제 (최소 $5)\n",
    "   - Settings -> Billing \n",
    "  \n",
    "![anthropic_apikey3.png](figures/anthropic_apikey3.png)\n",
    "  - 설문조사 후 카드 등록한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8174de-b6a1-423b-8cce-271917ae8dc6",
   "metadata": {},
   "source": [
    "## Anthropic의 Claude 모델 사용\n",
    "- 모델 확인: https://docs.anthropic.com/en/docs/about-claude/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8193ce-fa8b-4208-a7d9-af794044e61c",
   "metadata": {},
   "source": [
    "### Claude 모델 사용을 위한 package 설치\n",
    "\n",
    "```bash\n",
    "pip install langchain-anthropic -qU\n",
    "```\n",
    "- `anthropic`package도 같이 설치 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e3a23f-c491-4245-9013-83c5706fd4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0298545f-64b9-41aa-a375-7296030bb108",
   "metadata": {},
   "source": [
    "### Langchain-antropic 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba755283-5d24-464d-8c81-21d496100256",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "\"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      4\u001b[39m model=\u001b[33m\"\u001b[39m\u001b[33mclaude-sonnet-4-20250514\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m llm = ChatAnthropic(\n\u001b[32m      6\u001b[39m     model=model,\n\u001b[32m      7\u001b[39m     temperature=\u001b[32m0.2\u001b[39m,\n\u001b[32m      8\u001b[39m     max_tokens=\u001b[32m1024\u001b[39m,\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m result = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAnthropic의 LLM 모델은 어떤 것이 있는지 알려주고 간단한 설명도 부탁해.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/langchain_anthropic/chat_models.py:1455\u001b[39m, in \u001b[36mChatAnthropic._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1453\u001b[39m payload = \u001b[38;5;28mself\u001b[39m._get_request_payload(messages, stop=stop, **kwargs)\n\u001b[32m   1454\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1455\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1456\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m anthropic.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1457\u001b[39m     _handle_anthropic_bad_request(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/langchain_anthropic/chat_models.py:1316\u001b[39m, in \u001b[36mChatAnthropic._create\u001b[39m\u001b[34m(self, payload)\u001b[39m\n\u001b[32m   1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.beta.messages.create(**payload)\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/anthropic/_utils/_utils.py:283\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/anthropic/resources/messages/messages.py:978\u001b[39m, in \u001b[36mMessages.create\u001b[39m\u001b[34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[32m    972\u001b[39m     warnings.warn(\n\u001b[32m    973\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    974\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    975\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    976\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/v1/messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop_sequences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthinking\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    995\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    996\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsStreaming\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/anthropic/_base_client.py:1290\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1278\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1285\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1286\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1287\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1288\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1289\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1290\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/anthropic/_base_client.py:1009\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1006\u001b[39m options = \u001b[38;5;28mself\u001b[39m._prepare_options(options)\n\u001b[32m   1008\u001b[39m remaining_retries = max_retries - retries_taken\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m request = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[38;5;28mself\u001b[39m._prepare_request(request)\n\u001b[32m   1012\u001b[39m kwargs: HttpxSendArgs = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/anthropic/_base_client.py:505\u001b[39m, in \u001b[36mBaseClient._build_request\u001b[39m\u001b[34m(self, options, retries_taken)\u001b[39m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    503\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected JSON data type, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(json_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, cannot merge with `extra_body`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m headers = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m params = _merge_mappings(\u001b[38;5;28mself\u001b[39m.default_query, options.params)\n\u001b[32m    507\u001b[39m content_type = headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/anthropic/_base_client.py:446\u001b[39m, in \u001b[36mBaseClient._build_headers\u001b[39m\u001b[34m(self, options, retries_taken)\u001b[39m\n\u001b[32m    436\u001b[39m custom_headers = options.headers \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    437\u001b[39m headers_dict = _merge_mappings(\n\u001b[32m    438\u001b[39m     {\n\u001b[32m    439\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mx-stainless-timeout\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(options.timeout.read)\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m     custom_headers,\n\u001b[32m    445\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheaders_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[38;5;66;03m# headers are case-insensitive while dictionaries are not.\u001b[39;00m\n\u001b[32m    449\u001b[39m headers = httpx.Headers(headers_dict)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lang_env/lib/python3.12/site-packages/anthropic/_client.py:196\u001b[39m, in \u001b[36mAnthropic._validate_headers\u001b[39m\u001b[34m(self, headers, custom_headers)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(custom_headers.get(\u001b[33m\"\u001b[39m\u001b[33mAuthorization\u001b[39m\u001b[33m\"\u001b[39m), Omit):\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    197\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    198\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\""
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic# , Anthropic 지원하는 모델이 다른 것 같다.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "model=\"claude-sonnet-4-20250514\"\n",
    "llm = ChatAnthropic(\n",
    "    model=model,\n",
    "    temperature=0.2,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "result = llm.invoke(\"Anthropic의 LLM 모델은 어떤 것이 있는지 알려주고 간단한 설명도 부탁해.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb8db47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic의 주요 LLM 모델은 다음과 같습니다:\n",
      "\n",
      "1. Claude\n",
      "- 가장 최신 대화형 AI 모델\n",
      "- 높은 윤리성과 안전성 강조\n",
      "- 정확하고 유연한 대화 능력\n",
      "- 현재 Claude 3 버전 출시\n",
      "\n",
      "2. Claude 2\n",
      "- Claude의 이전 버전\n",
      "- 더 넓은 컨텍스트 이해 능력\n",
      "- 복잡한 작업 처리에 강점\n",
      "\n",
      "3. Claude Instant\n",
      "- 더 가벼운 버전의 모델\n",
      "- 빠른 응답 속도\n",
      "- 간단한 작업에 최적화\n",
      "\n",
      "이 중 Claude 3가 현재 가장 최신이고 성능이 우수한 모델입니다.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161bc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1684d4b-a5f9-4e24-8263-b30d997833b0",
   "metadata": {},
   "source": [
    "# Ollama 모델 사용\n",
    "\n",
    "Ollama는 로컬 환경에서 오픈소스 LLM을 쉽게 실행할 수 있도록 지원하는 플랫폼이다.\n",
    "\n",
    "- 주요특징\n",
    "\n",
    "  - **다양한 모델 지원**: Llama 3, Mistral, Phi 3 등 여러 오픈소스 LLM을 지원.\n",
    "  - **편리한 모델 설치 및 실행**: 간단한 명령어로 모델을 다운로드하고 실행할 수 있습니다.\n",
    "  - **운영체제 호환성**: macOS, Windows, Linux 등 다양한 운영체제에서 사용 가능하다.\n",
    "\n",
    "## 설치\n",
    "- https://ollama.com/download 에서 운영체제에 맞는 버전을 설치\n",
    "-  Windows 버전은 특별한 설정 없이 바로 install 실행하면 된다.\n",
    "\n",
    "## 모델 검색\n",
    "- https://ollama.com/search\n",
    "- 모델을 검색한 후 상세페이지로 이동하면 해당 모델을 실행할 수있는 명령어가 나온다.\n",
    "\n",
    "![ollama_down.png](figures/ollama_down.png)\n",
    "\n",
    "\n",
    "## 실행 명령어\n",
    "- `ollama pull 모델명`\n",
    "  - 모델을 다운로드 받는다. (다운로드만 받고 실행은 하지 않은다.)\n",
    "- `ollama run 모델명`\n",
    "  - 모델을 실행한다. \n",
    "  - 최초 실행시 모델을 다운로드 받는다.\n",
    "  - 명령프롬프트 상에서 `프롬프트`를 입력하면 모델의 응답을 받을 수 있다.\n",
    "\n",
    "## Python/Langchain API\n",
    "- ollama api\n",
    "  - https://github.com/ollama/ollama-python\n",
    "- langchain-ollama\n",
    "  - https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "- 설치\n",
    "  - `pip install langchain-ollama`\n",
    "  - `ollama` package도 같이 설치 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c2254-7dd3-48b1-9de4-d25e278a2266",
   "metadata": {},
   "source": [
    "## Langchain-ollama 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410c58d-5d0d-42a8-a2d6-0dfa0f7772f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama run llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd2dbcc-d3f6-4e30-86dd-f33db4f69a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"qwen3:0.6b\"\n",
    ")\n",
    "response = model.invoke(\"Ollama의 LLM 모델은 어떤 것이 있는지 알려주고 간단한 설명도 부탁해.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "832b1cd0-e201-478f-85ae-f7ab40c778dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking about the LLM models available on Ollama and wants a simple explanation. Let me start by recalling what I know about Ollama. From what I remember, Ollama is a large-scale open-source language model, right? It's designed for various use cases like language processing, text generation, and other tasks.\n",
      "\n",
      "First, I need to list the models available on Ollama. I think there are several. Maybe they have a main model, like a large language model, and then maybe some specific ones. Wait, I should check my knowledge here. Ollama's main model is probably a large language model, but sometimes they offer other variants. For example, maybe they have a specific model like a transformer-based model or something similar.\n",
      "\n",
      "Wait, I'm a bit fuzzy with the exact models. Let me think. Ollama's main model is probably a large language model, but they might also have other models. For example, maybe they have a specific model for tasks like text generation, or they have a model optimized for certain tasks. Also, maybe there are other models available, like those developed by different teams or specific industries.\n",
      "\n",
      "I should mention the main model and then list the other available models briefly. Also, make sure to explain that Ollama is a large-scale open-source project. Oh, and maybe mention that they have a community and developers who can help. Oh, and that Ollama is designed for different use cases, like language processing, text generation, etc.\n",
      "\n",
      "Wait, but I need to be concise. Let me structure this. Start by stating the main model, then list other available models with brief descriptions. Finally, explain that Ollama is an open-source project with community support. Double-check that I'm not missing any models. Yes, that seems right.\n",
      "</think>\n",
      "\n",
      "**Ollama의 LLM 모델**은 다음과 같은 기반 모델을 제공합니다:  \n",
      "\n",
      "1. **main model**:  \n",
      "   - Ollama의 핵심 LLM 모델은 전통적 LLM 기반 모델, 예를 들어 **T5, BERT, GPT-3** 등으로 구현되어 있습니다.  \n",
      "\n",
      "2. **특수 활용 모델**:  \n",
      "   - **Text Generation**: 다양한 문장 생성 기능을 지원하는 모델, 예를 들어 \"text generation\" 또는 \"text synthesis\" 기능.  \n",
      "   - **Language Processing**: 문법 분석, 번역, 자연어 처리 등 다양한 언어 처리 기능.  \n",
      "\n",
      "**소식**:  \n",
      "- Ollama는 **open-source** 프로젝트로, 개발자 및 개발자료를 공개하며, 다양한 시각적 측면에서 활용 가능한 모델을 포함합니다.  \n",
      "\n",
      "이러한 기반 모델을 사용할 수 있어요! 😊\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b1d44",
   "metadata": {},
   "source": [
    "# Gemini\n",
    "- 모델: https://ai.google.dev/gemini-api/docs/models?hl=ko\n",
    "- 가격정책: https://ai.google.dev/gemini-api/docs/pricing?hl=ko\n",
    "\n",
    "## API Key 생성\n",
    "\n",
    "1. https://aistudio.google.com/\n",
    "    - 연결 후 로그인(구글계정)\n",
    "2. Get API Key 클릭\n",
    "   \n",
    "    ![img](figures/gemini_api1.png)\n",
    "\n",
    "3. `API Key 만들기` 선택\n",
    "4. 프로젝트 선택 후 `기존 프로젝트에서 API 키 만들기` 선택\n",
    "\n",
    "## 환경변수\n",
    "- `GOOGLE_API_KEY` 환경변수에 생성된 API Key를 등록한다.\n",
    "## 설치\n",
    "- `pip install langchain_google_genai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fde1d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # .env 파일에서 환경변수 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b2e8fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-preview-05-20\"\n",
    ")\n",
    "response = model.invoke(\"Google의 LLM 모델은 어떤 것이 있는지 알려주고 간단한 설명도 부탁해.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18500ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google은 AI 연구 분야에서 오랜 역사를 가지고 있으며, 다양한 대규모 언어 모델(LLM)을 개발해 왔습니다. 주요 모델들과 그에 대한 간단한 설명은 다음과 같습니다.\n",
      "\n",
      "1.  **Gemini (제미나이)**\n",
      "    *   **설명:** 현재 Google의 가장 강력하고 최신 플래그십 LLM입니다. 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 정보를 이해하고 추론하며 생성할 수 있는 **다중 모드(Multimodal)** 능력을 갖춘 것이 특징입니다. 다양한 크기(Ultra, Pro, Nano 등)로 제공되어 스마트폰부터 데이터센터까지 폭넓게 활용됩니다.\n",
      "    *   **활용:** Google 검색, Google Workspace, 안드로이드, 그리고 개발자들을 위한 API 등 Google의 다양한 제품과 서비스의 핵심 기반으로 사용되고 있으며, 기존의 'Bard' 서비스도 이제 'Gemini'라는 이름으로 통합되어 이 모델을 사용합니다.\n",
      "\n",
      "2.  **PaLM (팜: Pathways Language Model)**\n",
      "    *   **설명:** Gemini 이전에 Google의 주요 대규모 언어 모델이었습니다. 특히 **PaLM 2**는 Gemini가 나오기 전까지 Google의 많은 AI 서비스와 개발자 도구에 활용되었던 모델입니다. 대화, 텍스트 생성, 요약, 번역, 코드 생성 등 다양한 언어 관련 작업을 수행할 수 있었습니다. Gemini의 개발에 중요한 기반이 되었습니다.\n",
      "\n",
      "3.  **LaMDA (람다: Language Model for Dialogue Applications)**\n",
      "    *   **설명:** 대화 애플리케이션에 특화된 언어 모델로, 인간과 자연스럽고 유창한 대화를 나누는 데 중점을 두었습니다. 개방형 대화에서 미묘한 뉘앙스를 이해하고, 일관성 있고 흥미로운 답변을 생성하는 데 뛰어난 능력을 보였습니다. 초기 'Bard' 서비스의 기반 모델 중 하나였습니다.\n",
      "\n",
      "4.  **Bard (바드)**\n",
      "    *   **설명:** Bard는 **모델의 이름이 아니라 Google이 제공했던 대화형 AI 서비스의 이름**이었습니다. 처음에는 LaMDA를 기반으로 시작하여, 이후 PaLM 2를 거쳐 현재는 최신 Gemini 모델을 기반으로 작동합니다. 2024년 2월, Google은 Bard를 **'Gemini'라는 이름으로 공식적으로 변경**하여, 이제는 서비스 이름과 기반 모델 이름이 동일하게 되었습니다. 즉, 현재 'Bard'라는 이름의 서비스는 더 이상 존재하지 않으며, 'Gemini'가 그 역할을 이어받았습니다.\n",
      "\n",
      "**요약:**\n",
      "Google의 LLM은 **LaMDA (대화 특화) → PaLM (대규모 언어 모델) → Gemini (최신, 다중 모드)** 순으로 발전해 왔습니다. 'Bard'는 이 모델들을 활용하여 제공되던 대화형 AI 서비스의 이름이었고, 현재는 이 서비스 자체도 **'Gemini'**라는 이름으로 통합되어 Google의 최신 Gemini 모델을 사용하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5411db7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Google은 AI 연구 분야에서 오랜 역사를 가지고 있으며, 다양한 대규모 언어 모델(LLM)을 개발해 왔습니다. 주요 모델들과 그에 대한 간단한 설명은 다음과 같습니다.\n",
       "\n",
       "1.  **Gemini (제미나이)**\n",
       "    *   **설명:** 현재 Google의 가장 강력하고 최신 플래그십 LLM입니다. 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 정보를 이해하고 추론하며 생성할 수 있는 **다중 모드(Multimodal)** 능력을 갖춘 것이 특징입니다. 다양한 크기(Ultra, Pro, Nano 등)로 제공되어 스마트폰부터 데이터센터까지 폭넓게 활용됩니다.\n",
       "    *   **활용:** Google 검색, Google Workspace, 안드로이드, 그리고 개발자들을 위한 API 등 Google의 다양한 제품과 서비스의 핵심 기반으로 사용되고 있으며, 기존의 'Bard' 서비스도 이제 'Gemini'라는 이름으로 통합되어 이 모델을 사용합니다.\n",
       "\n",
       "2.  **PaLM (팜: Pathways Language Model)**\n",
       "    *   **설명:** Gemini 이전에 Google의 주요 대규모 언어 모델이었습니다. 특히 **PaLM 2**는 Gemini가 나오기 전까지 Google의 많은 AI 서비스와 개발자 도구에 활용되었던 모델입니다. 대화, 텍스트 생성, 요약, 번역, 코드 생성 등 다양한 언어 관련 작업을 수행할 수 있었습니다. Gemini의 개발에 중요한 기반이 되었습니다.\n",
       "\n",
       "3.  **LaMDA (람다: Language Model for Dialogue Applications)**\n",
       "    *   **설명:** 대화 애플리케이션에 특화된 언어 모델로, 인간과 자연스럽고 유창한 대화를 나누는 데 중점을 두었습니다. 개방형 대화에서 미묘한 뉘앙스를 이해하고, 일관성 있고 흥미로운 답변을 생성하는 데 뛰어난 능력을 보였습니다. 초기 'Bard' 서비스의 기반 모델 중 하나였습니다.\n",
       "\n",
       "4.  **Bard (바드)**\n",
       "    *   **설명:** Bard는 **모델의 이름이 아니라 Google이 제공했던 대화형 AI 서비스의 이름**이었습니다. 처음에는 LaMDA를 기반으로 시작하여, 이후 PaLM 2를 거쳐 현재는 최신 Gemini 모델을 기반으로 작동합니다. 2024년 2월, Google은 Bard를 **'Gemini'라는 이름으로 공식적으로 변경**하여, 이제는 서비스 이름과 기반 모델 이름이 동일하게 되었습니다. 즉, 현재 'Bard'라는 이름의 서비스는 더 이상 존재하지 않으며, 'Gemini'가 그 역할을 이어받았습니다.\n",
       "\n",
       "**요약:**\n",
       "Google의 LLM은 **LaMDA (대화 특화) → PaLM (대규모 언어 모델) → Gemini (최신, 다중 모드)** 순으로 발전해 왔습니다. 'Bard'는 이 모델들을 활용하여 제공되던 대화형 AI 서비스의 이름이었고, 현재는 이 서비스 자체도 **'Gemini'**라는 이름으로 통합되어 Google의 최신 Gemini 모델을 사용하고 있습니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(response.content)  # 응답 메시지를 Markdown 형식으로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38d3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
